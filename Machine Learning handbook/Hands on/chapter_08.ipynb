{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 8: Linear classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ![picture](pictures/title.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8.1 Logistic Regression\n",
    "\n",
    "### Introduction\n",
    "In this chapter, we delve into linear classification, specifically focusing on logistic regression as a generalized linear model. Logistic regression aims to construct a linear classifier $( f: \\mathbb{R}^d \\rightarrow {-1, +1} )$ by modeling the posterior probabilities $( p(y|\\mathbf{x}) $). We introduce the sigmoid function $( \\sigma(t) = (1 + e^{-t})^{-1} )$ as the transfer function.\n",
    "\n",
    "### Logistic Regression Model\n",
    "The logistic regression model is defined as follows:\n",
    "$$ [ p(y|\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x}) ]$$\n",
    "where $( \\mathbf{w} = [w_0, w_1, \\ldots, w_d] )$ represents a set of weights and $( \\mathbf{x} = [x_0=1, x_1, \\ldots, x_d] )$ is the input space. Here, $( \\mathbf{x}_0=1 )$ is added to simplify notation, expanding the input space to $( \\mathcal{X} = \\mathbb{R}^{d+1} )$.\n",
    "\n",
    "### Prediction of Class Labels\n",
    "The logistic regression model returns probabilities instead of explicit predictions of 0 or 1. To obtain class predictions, we use a threshold of 0.5. If $( P(Y=1|\\mathbf{x},\\mathbf{w}) \\geq 0.5 )$, the data point is classified as positive $(( \\hat{y} = 1 ))$; otherwise, it is classified as negative $(( \\hat{y} = 0 ))$.\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "The model parameters $( \\mathbf{w} )$ are learned through maximum likelihood estimation. The gradient descent algorithm is employed, and the gradient of the negative log-likelihood for each sample is given by:\n",
    "$$[ -\\frac{\\partial \\mathcal{L}_i(\\mathbf{w})}{\\partial w_j} = (\\sigma(\\mathbf{x}_i^T\\mathbf{w}) - y_i)x_{ij} ]$$\n",
    "\n",
    "### Problems Related to Euclidean Distance Minimization\n",
    "An alternative approach to classification using logistic regression is discussed, highlighting the challenges associated with Euclidean distance minimization.\n",
    "\n",
    "## 8.2 Simple Bayes Classifier\n",
    "\n",
    "### Introduction\n",
    "The Simple Bayes classifier is introduced as a generative approach for prediction. Unlike discriminant approaches, Simple Bayes learns the joint distribution $( p(\\mathbf{x}, y) = p(\\mathbf{x}|y)p(y) )$. The assumption of independence among features given the label is illustrated through a graphical model.\n",
    "\n",
    "### Binary Features and Linear Classification\n",
    "For binary features, the Simple Bayes classifier is shown to be a linear classifier. The decision rule for labeling a point as class $( c )$ is derived, and it is demonstrated that Simple Bayes is a linear classifier in the case of binary features.\n",
    "\n",
    "### Continuous Simple Bayes\n",
    "For continuous features, the Simple Bayes classifier is extended using the Gaussian distribution for $( p(\\mathbf{x}|y) )$, with different mean and variance parameters for each feature and class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
